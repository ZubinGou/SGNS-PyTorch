{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T13:36:08.591535Z",
     "start_time": "2021-03-08T13:36:08.583358Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T13:36:09.578435Z",
     "start_time": "2021-03-08T13:36:09.575113Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_wordsim(wordvec):\n",
    "    p_ws353 = wordvec.evaluate_word_pairs(datapath('wordsim353.tsv'))[1][0]\n",
    "    p_rw = wordvec.evaluate_word_pairs(\"word2vec/rw/rw_clean.txt\")[1][0]\n",
    "    p_sl999 = wordvec.evaluate_word_pairs(datapath('simlex999.txt'))[1][0]\n",
    "    print(\"WS353:\", p_ws353)\n",
    "    print(\"RW:\", p_rw)\n",
    "    print(\"SL999\", p_sl999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-08T12:45:57.775Z"
    }
   },
   "source": [
    "## Pretrained GoogleNews-300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T13:20:41.004257Z",
     "start_time": "2021-03-08T13:20:06.626647Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab: 3000000\n",
      "vec_dim: 300\n"
     ]
    }
   ],
   "source": [
    "wv_google = KeyedVectors.load_word2vec_format('dataset/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "print(\"vocab:\", len(wv_google.vocab))\n",
    "print(\"vec_dim:\", wv_google.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T13:21:59.029201Z",
     "start_time": "2021-03-08T13:21:57.987236Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WS353: 0.6589215888009288\n",
      "RW: 0.5525559901031721\n",
      "SL999 0.43607859778335434\n"
     ]
    }
   ],
   "source": [
    "evaluate_wordsim(wv_google)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-06T02:34:51.871196Z",
     "start_time": "2021-03-06T02:34:51.858070Z"
    }
   },
   "source": [
    "## Train by gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T14:32:02.775771Z",
     "start_time": "2021-03-08T14:31:58.660577Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = api.load(\"text8\")\n",
    "\n",
    "with open(\"dataset/text8.txt\", \"w\") as f:\n",
    "    for data in dataset:\n",
    "        f.write(\" \".join(data) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提取text8部分：\n",
    "```sh\n",
    "$ head -i 300 text8.txt > text8_300.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T12:28:26.739730Z",
     "start_time": "2021-03-08T12:27:17.689878Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab: 71290\n",
      "vec_dim: 100\n"
     ]
    }
   ],
   "source": [
    "dataset = api.load(\"text8\")\n",
    "model = Word2Vec(dataset)\n",
    "print(\"vocab:\", len(model.wv.vocab))\n",
    "print(\"vec_dim:\", model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T12:33:04.443293Z",
     "start_time": "2021-03-08T12:33:04.012247Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.623768051438205\n",
      "0.3198280325425669\n",
      "0.24994655821500755\n"
     ]
    }
   ],
   "source": [
    "evaluate_wordsim(model.wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train by SGNS-PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T14:29:15.957452Z",
     "start_time": "2021-03-08T13:47:38.421346Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "len data: 17005207\n",
      "epoch= 1, batch=    0: sp=0.614 0.369  pair/sec = 91120.71 loss=0.073\n",
      "epoch= 1, batch=10000: sp=0.610 0.366  pair/sec = 10931.75 loss=0.381\n",
      "epoch= 1, batch=20000: sp=0.613 0.366  pair/sec = 10947.02 loss=0.708\n",
      "epoch= 1, batch=30000: sp=0.609 0.368  pair/sec = 10948.62 loss=0.856\n",
      "epoch= 1, batch=40000: sp=0.618 0.370  pair/sec = 11021.52 loss=0.850\n",
      "epoch= 1, batch=50000: sp=0.622 0.374  pair/sec = 9802.78 loss=0.670\n",
      "epoch= 1, batch=60000: sp=0.622 0.373  pair/sec = 10863.37 loss=0.778\n",
      "epoch= 1, batch=70000: sp=0.624 0.376  pair/sec = 11011.65 loss=0.657\n",
      "epoch= 1, batch=80000: sp=0.629 0.379  pair/sec = 10943.09 loss=0.730\n",
      "epoch= 1, batch=90000: sp=0.623 0.382  pair/sec = 10665.93 loss=0.568\n",
      "epoch= 1, batch=100000: sp=0.622 0.381  pair/sec = 9668.13 loss=0.707\n",
      "epoch= 1, batch=110000: sp=0.620 0.385  pair/sec = 10997.14 loss=0.791\n",
      "epoch= 1, batch=120000: sp=0.621 0.388  pair/sec = 10948.56 loss=0.743\n",
      "epoch= 1, batch=130000: sp=0.617 0.389  pair/sec = 10962.46 loss=0.650\n",
      "epoch= 1, batch=140000: sp=0.616 0.387  pair/sec = 10853.96 loss=0.792\n",
      "epoch= 1, batch=150000: sp=0.617 0.393  pair/sec = 9491.06 loss=0.722\n",
      "epoch= 1, batch=160000: sp=0.617 0.395  pair/sec = 10666.19 loss=0.751\n",
      "epoch= 1, batch=170000: sp=0.619 0.396  pair/sec = 10809.44 loss=0.693\n",
      "epoch= 1, batch=180000: sp=0.619 0.396  pair/sec = 10757.78 loss=0.624\n",
      "epoch= 1, batch=190000: sp=0.621 0.397  pair/sec = 10762.67 loss=0.676\n",
      "epoch= 1, batch=200000: sp=0.622 0.398  pair/sec = 9570.95 loss=0.695\n",
      "epoch= 1, batch=210000: sp=0.629 0.399  pair/sec = 10977.68 loss=0.717\n",
      "\n",
      "epoch= 2, batch=    0: sp=0.631 0.398  pair/sec = 91765.56 loss=0.077\n",
      "epoch= 2, batch=10000: sp=0.635 0.401  pair/sec = 10578.39 loss=0.732\n",
      "epoch= 2, batch=20000: sp=0.637 0.403  pair/sec = 10933.05 loss=0.677\n",
      "epoch= 2, batch=30000: sp=0.637 0.403  pair/sec = 10993.21 loss=0.496\n",
      "epoch= 2, batch=40000: sp=0.635 0.402  pair/sec = 10965.48 loss=0.747\n",
      "epoch= 2, batch=50000: sp=0.637 0.401  pair/sec = 9749.48 loss=0.729\n",
      "epoch= 2, batch=60000: sp=0.634 0.399  pair/sec = 10748.01 loss=0.755\n",
      "epoch= 2, batch=70000: sp=0.638 0.401  pair/sec = 10765.16 loss=0.763\n",
      "epoch= 2, batch=80000: sp=0.639 0.402  pair/sec = 10702.73 loss=0.666\n",
      "epoch= 2, batch=90000: sp=0.639 0.400  pair/sec = 10929.50 loss=0.683\n",
      "epoch= 2, batch=100000: sp=0.639 0.398  pair/sec = 9859.26 loss=0.529\n",
      "epoch= 2, batch=110000: sp=0.640 0.399  pair/sec = 11000.12 loss=0.671\n",
      "epoch= 2, batch=120000: sp=0.641 0.399  pair/sec = 10861.99 loss=0.666\n",
      "epoch= 2, batch=130000: sp=0.642 0.399  pair/sec = 10950.50 loss=0.708\n",
      "epoch= 2, batch=140000: sp=0.644 0.398  pair/sec = 10893.25 loss=0.730\n",
      "epoch= 2, batch=150000: sp=0.645 0.401  pair/sec = 9716.51 loss=0.741\n",
      "epoch= 2, batch=160000: sp=0.645 0.402  pair/sec = 10600.14 loss=0.754\n",
      "epoch= 2, batch=170000: sp=0.641 0.403  pair/sec = 10950.56 loss=0.738\n",
      "epoch= 2, batch=180000: sp=0.641 0.403  pair/sec = 11000.89 loss=0.752\n",
      "epoch= 2, batch=190000: sp=0.642 0.406  pair/sec = 10997.12 loss=0.673\n",
      "epoch= 2, batch=200000: sp=0.638 0.405  pair/sec = 9710.42 loss=0.699\n",
      "epoch= 2, batch=210000: sp=0.638 0.405  pair/sec = 10665.89 loss=0.663\n",
      "epoch= 2, batch=220000: sp=0.638 0.406  pair/sec = 10940.30 loss=0.771\n",
      "epoch= 2, batch=230000: sp=0.633 0.407  pair/sec = 10871.00 loss=0.763\n",
      "epoch= 2, batch=240000: sp=0.635 0.408  pair/sec = 10846.60 loss=0.743\n",
      "epoch= 2, batch=250000: sp=0.636 0.409  pair/sec = 9777.09 loss=0.706\n",
      "epoch= 2, batch=260000: sp=0.636 0.407  pair/sec = 10988.26 loss=0.779\n",
      "epoch= 2, batch=270000: sp=0.638 0.410  pair/sec = 10872.69 loss=0.766\n",
      "epoch= 2, batch=280000: sp=0.638 0.410  pair/sec = 10837.28 loss=0.725\n",
      "epoch= 2, batch=290000: sp=0.643 0.410  pair/sec = 10885.61 loss=0.741\n",
      "\n",
      "epoch= 3, batch=    0: sp=0.644 0.410  pair/sec = 92695.30 loss=0.074\n",
      "epoch= 3, batch=10000: sp=0.644 0.412  pair/sec = 10880.96 loss=0.737\n",
      "epoch= 3, batch=20000: sp=0.646 0.414  pair/sec = 10929.24 loss=0.673\n",
      "epoch= 3, batch=30000: sp=0.646 0.412  pair/sec = 10770.30 loss=0.478\n",
      "epoch= 3, batch=40000: sp=0.644 0.411  pair/sec = 10750.37 loss=0.745\n",
      "epoch= 3, batch=50000: sp=0.644 0.409  pair/sec = 9806.29 loss=0.726\n",
      "epoch= 3, batch=60000: sp=0.642 0.407  pair/sec = 10961.06 loss=0.748\n",
      "epoch= 3, batch=70000: sp=0.646 0.408  pair/sec = 10871.39 loss=0.744\n",
      "epoch= 3, batch=80000: sp=0.646 0.409  pair/sec = 10650.55 loss=0.657\n",
      "epoch= 3, batch=90000: sp=0.645 0.407  pair/sec = 10704.30 loss=0.682\n",
      "epoch= 3, batch=100000: sp=0.645 0.405  pair/sec = 9573.75 loss=0.523\n",
      "epoch= 3, batch=110000: sp=0.645 0.406  pair/sec = 10601.76 loss=0.668\n",
      "epoch= 3, batch=120000: sp=0.649 0.406  pair/sec = 10941.81 loss=0.650\n",
      "epoch= 3, batch=130000: sp=0.649 0.407  pair/sec = 11038.63 loss=0.714\n",
      "epoch= 3, batch=140000: sp=0.651 0.406  pair/sec = 10904.09 loss=0.717\n",
      "epoch= 3, batch=150000: sp=0.652 0.409  pair/sec = 9601.10 loss=0.741\n",
      "epoch= 3, batch=160000: sp=0.653 0.410  pair/sec = 10697.60 loss=0.748\n",
      "epoch= 3, batch=170000: sp=0.650 0.411  pair/sec = 10887.27 loss=0.731\n",
      "epoch= 3, batch=180000: sp=0.649 0.410  pair/sec = 10891.27 loss=0.748\n",
      "epoch= 3, batch=190000: sp=0.648 0.412  pair/sec = 10666.88 loss=0.667\n",
      "epoch= 3, batch=200000: sp=0.646 0.412  pair/sec = 9580.67 loss=0.682\n",
      "epoch= 3, batch=210000: sp=0.644 0.412  pair/sec = 10736.35 loss=0.659\n",
      "epoch= 3, batch=220000: sp=0.647 0.412  pair/sec = 10960.56 loss=0.767\n",
      "epoch= 3, batch=230000: sp=0.644 0.413  pair/sec = 10843.09 loss=0.753\n",
      "epoch= 3, batch=240000: sp=0.643 0.414  pair/sec = 10886.26 loss=0.738\n",
      "epoch= 3, batch=250000: sp=0.646 0.414  pair/sec = 9801.33 loss=0.684\n",
      "epoch= 3, batch=260000: sp=0.645 0.412  pair/sec = 10718.44 loss=0.770\n",
      "epoch= 3, batch=270000: sp=0.646 0.415  pair/sec = 10653.46 loss=0.762\n",
      "epoch= 3, batch=280000: sp=0.648 0.415  pair/sec = 10715.18 loss=0.725\n",
      "epoch= 3, batch=290000: sp=0.651 0.415  pair/sec = 10645.31 loss=0.738\n",
      "\n",
      "epoch= 4, batch=    0: sp=0.651 0.415  pair/sec = 91711.27 loss=0.073\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-539fa5c8f97a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# wv = Word2VecTrainer(\"dataset/text8.txt\", saved_model_path=\"\", output_file=\"sgns.vec\") # emb_dim=100, vocab=30000, SparseAdam, lr=0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# wv = Word2VecTrainer(\"dataset/text8_100.txt\", output_file=\"sgns_mini.vec\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/code/SGNS-PyTorch/word2vec/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip_gram_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_u\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_num\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/optim/sparse_adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoalesce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# the update is non-linear so indices must be unique\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0mgrad_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mgrad_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "from gensim.models import Word2Vec\n",
    "import tqdm\n",
    "\n",
    "from word2vec.trainer import Word2VecTrainer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "wv = Word2VecTrainer(\"dataset/text8.txt\", saved_model_path=\"tmp/skipgram.epoch1.batch50000\", output_file=\"sgns.vec\") # emb_dim=100, vocab=50000, SparseAdam, lr=0.001\n",
    "\n",
    "wv.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T14:33:06.803529Z",
     "start_time": "2021-03-08T14:32:59.492551Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vocab size 50000\n",
      "WS353: 0.660792652633121\n",
      "RW: 0.3430154080998551\n",
      "SL999 0.2649420256825831\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "wv = KeyedVectors.load_word2vec_format(\"sgns.vec\", binary=False)\n",
    "vocab = list(wv.vocab.keys())\n",
    "print(\"Loaded vocab size %i\" % len(vocab))\n",
    "evaluate_wordsim(wv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
